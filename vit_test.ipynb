{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e065c3-fcd2-4d20-9df6-eca4967ef4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/crc/c/conda/23.5.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTConfig, ViTForMaskedImageModeling\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from datasets import load_dataset,load_metric\n",
    "\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d1c3c-ec71-4d67-bfb0-49f7d316d66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# url = \"http://farm3.staticflickr.com/2066/1798910782_5536af8767_z.jpg\"\n",
    "# url = \"http://farm1.staticflickr.com/184/399924547_98e6cef97a_z.jpg\"\n",
    "# url = \"http://farm1.staticflickr.com/128/318959350_1a39aae18c_z.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "def load_data(): \n",
    "    dataset = load_dataset(\"mrm8488/ImageNet1K-val\")\n",
    "    dataset = dataset['train']\n",
    "    splits = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    test_ds = splits['test']\n",
    "    splits = splits['train'].train_test_split(test_size=0.1, seed=42)\n",
    "    train_ds = splits['train']\n",
    "    val_ds = splits['test']\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "train_ds, val_ds, test_ds = load_data()\n",
    "\n",
    "image = train_ds[3]['image']\n",
    "\n",
    "# pretrained_name = 'google/vit-base-patch16-224'\n",
    "pretrained_name = 'vit-base-patch16-224-finetuned-imageneteval/checkpoint-60'\n",
    "# pretrained_name = 'openai/clip-vit-base-patch32'\n",
    "config = ViTConfig.from_pretrained(pretrained_name)\n",
    "processor = ViTImageProcessor.from_pretrained(pretrained_name)\n",
    "# get mean and std to unnormalize the processed images\n",
    "mean, std = processor.image_mean, processor.image_std\n",
    "\n",
    "pred_model = ViTForImageClassification.from_pretrained(pretrained_name)\n",
    "pred_model.to(device)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "outputs = pred_model(**inputs, output_hidden_states=True)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", pred_model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3aa55-e056-4759-a593-a72de1b5f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pyx_prime(model, outputs):\n",
    "    \"\"\"\n",
    "    Obtain p(y|x) and p(y|x'), where x' is the input with the ith entry missing.\n",
    "    Args:\n",
    "        model: a ViT model\n",
    "        outputs: the outputs of the ViT model given input x\n",
    "    Returns:\n",
    "        pyx: p(y|x)\n",
    "        pyx_prime: p(y|x')\n",
    "    \"\"\"\n",
    "    image_embeds = outputs.hidden_states[-1] # [N, L+1, d]\n",
    "    image_embeds = model.vit.layernorm(image_embeds) # [N, L+1, d]\n",
    "    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "    logits = model.classifier(image_embeds) # [N, L+1, 1000]\n",
    "    logits = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    pyx = logits[:, 0:1, :] # [N, 1, 1000]\n",
    "    pyx_prime = logits[:, 1:, :] # [N, L, 1000]\n",
    "    return pyx, pyx_prime\n",
    "\n",
    "def get_heatmap(pyx, pyx_prime):\n",
    "    \"\"\"\n",
    "    Given p(y|x) and p(y|x'), where x' is the input with the ith entry missing.\n",
    "    Args:\n",
    "        pyx: [N, 1, 1000]\n",
    "        pyx_prime: [N, L, 1000]\n",
    "\n",
    "    Returns: \n",
    "        heatmap: [N, 14, 14, 1000]\n",
    "    \"\"\"\n",
    "    res = (pyx - pyx_prime) # [N, L, N]\n",
    "    # res = pyx_prime\n",
    "    N_v, L, N_t = res.shape\n",
    "    res = (res>0).float() * res\n",
    "    \n",
    "    heatmap = res.reshape(N_v,14,14, N_t).detach().cpu().numpy()\n",
    "    return heatmap\n",
    "\n",
    "def unnormalize(img, mean, std):\n",
    "    mean = np.array(mean).reshape(1,1,3)\n",
    "    std = np.array(std).reshape(1,1,3)\n",
    "    return img * std + mean\n",
    "\n",
    "def convert_to_255_scale(img):\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "def unnormalize_and_255_scale(img, mean, std):\n",
    "    return convert_to_255_scale(unnormalize(img,mean,std))\n",
    "\n",
    "def show_superimposed(img, heatmap):\n",
    "    cv2_image = cv2.cvtColor(img.transpose(1,2,0), cv2.COLOR_RGB2BGR)\n",
    "    blur = cv2.GaussianBlur(heatmap,(13,13), 11)\n",
    "\n",
    "def normalize_and_rescale(heatmap):\n",
    "    max_value = np.max(heatmap)\n",
    "    min_value = np.min(heatmap)\n",
    "    heatmap_ft = (heatmap - min_value) / (max_value - min_value) # float point\n",
    "    return convert_to_255_scale(heatmap_ft) # int8\n",
    "\n",
    "def get_overlap(image, heatmap):\n",
    "    return cv2.addWeighted(heatmap, 0.5, image, 0.5, 0)\n",
    "\n",
    "def plot_overlap(image, heatmap):\n",
    "    overlap = get_overlap(image, heatmap)\n",
    "    plt.imshow(overlap)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return 0\n",
    "\n",
    "def plot_overlap_np(image, heatmap, img_mean, img_std):\n",
    "    shape = image.shape[:2]\n",
    "    heatmap = normalize_and_rescale(heatmap)\n",
    "    resized_heatmap = cv2.resize(heatmap, shape)\n",
    "    blur = cv2.blur(resized_heatmap ,(13,13), 11)\n",
    "    heatmap_img = cv2.applyColorMap(blur, cv2.COLORMAP_JET)\n",
    "    heatmap_img = cv2.cvtColor(heatmap_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image = unnormalize_and_255_scale(image, img_mean, img_std)\n",
    "    \n",
    "    plot_overlap(image, heatmap_img)\n",
    "    return image, heatmap_img\n",
    "\n",
    "pyx, pyx_prime = get_pyx_prime(pred_model, outputs)\n",
    "heatmap = get_heatmap(pyx, pyx_prime)[0,:,:,predicted_class_idx]\n",
    "img = inputs.pixel_values[0].cpu().numpy().transpose(1,2,0)\n",
    "\n",
    "image, heatmap_img = plot_overlap_np(img, heatmap, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b621f-43e5-49d7-ae74-b4a265811411",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.id2label[predicted_class_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd33d0-d9d6-4cec-8c41-4225e793e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pyx_prime[0,:,predicted_class_idx].reshape(14,14).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a741a81-d953-4ef5-811a-c0cdee1ce119",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f7f1def5-1b94-44eb-8e33-53c7672c70f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([-0.0796, -0.0848, -0.1004, -0.1096, -0.1098, -0.8484, -1.0187, -1.1403,\n",
       "        -1.3748, -1.3919], device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([41, 27, 55, 28, 14, 24, 25, 38, 53, 52], device='cuda:0'))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(-pyx_prime[0,:,predicted_class_idx], k=10)\n",
    "# torch.topk(-pyx_prime[0,:,:], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "af23a217-eaaf-419f-9e7e-6699f1550645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 1000])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyx_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "72ae999b-e0c0-4e94-a7fd-7387681cac1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6648, 0.1864, 0.1534, 0.4230, 0.5003, 0.2311, 0.2753, 0.2307, 0.4921,\n",
       "         0.6167, 0.9358, 0.4667, 0.5765, 0.3406],\n",
       "        [0.7313, 0.3677, 0.3738, 0.4811, 0.4511, 0.1885, 0.2276, 0.4310, 0.2549,\n",
       "         0.1728, 0.2518, 0.0900, 0.5523, 0.6250],\n",
       "        [0.6045, 0.6936, 0.5243, 0.7600, 0.3055, 0.4877, 0.6262, 0.4158, 0.1189,\n",
       "         0.2034, 0.1487, 0.2163, 0.2734, 0.1782],\n",
       "        [0.6791, 0.8337, 0.5728, 0.1679, 0.4762, 0.7462, 0.4919, 0.4401, 0.1413,\n",
       "         0.3464, 0.0770, 0.1961, 0.2623, 0.3292],\n",
       "        [0.1688, 0.2208, 0.1306, 0.1381, 0.1869, 0.2910, 0.4764, 0.8668, 0.1669,\n",
       "         0.0633, 0.0936, 0.2386, 0.4852, 0.4924],\n",
       "        [0.2015, 0.2300, 0.0885, 0.0780, 0.1484, 0.1327, 0.3209, 0.1737, 0.1333,\n",
       "         0.0803, 0.0756, 0.0994, 0.4112, 0.4168],\n",
       "        [0.2385, 0.1657, 0.1166, 0.1356, 0.5281, 0.4363, 0.2446, 0.1086, 0.0994,\n",
       "         0.0507, 0.1346, 0.2167, 0.5004, 0.2677],\n",
       "        [0.3029, 0.1601, 0.0129, 0.2558, 0.3674, 0.1925, 0.1957, 0.2768, 0.2492,\n",
       "         0.2386, 0.2380, 0.4125, 0.2830, 0.1933],\n",
       "        [0.1629, 0.2678, 0.0195, 0.3294, 0.5310, 0.3569, 0.3677, 0.2880, 0.1727,\n",
       "         0.0023, 0.2555, 0.1366, 0.5706, 0.3675],\n",
       "        [0.2260, 0.2031, 0.1061, 0.4990, 0.5116, 0.1776, 0.1991, 0.3731, 0.1176,\n",
       "         0.0621, 0.4261, 0.8107, 0.6188, 0.2361],\n",
       "        [0.1598, 0.0591, 0.2087, 0.3463, 0.2485, 0.3512, 0.4742, 0.3202, 0.1946,\n",
       "         0.0522, 0.4670, 0.8637, 0.6782, 0.2792],\n",
       "        [0.4809, 0.1663, 0.0746, 0.2635, 0.3257, 0.2870, 0.3762, 0.9353, 0.3696,\n",
       "         0.3110, 0.9357, 0.2606, 0.1969, 0.2376],\n",
       "        [0.1967, 0.1336, 0.3216, 0.9360, 0.9361, 0.9359, 0.4505, 0.3947, 0.9360,\n",
       "         0.9360, 0.1582, 0.2502, 0.2762, 0.3548],\n",
       "        [0.3136, 0.1498, 0.3594, 0.2965, 0.2743, 0.2055, 0.5819, 0.3309, 0.9360,\n",
       "         0.4859, 0.2013, 0.1996, 0.3802, 0.7844]], device='cuda:0',\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pyx - pyx_prime)[:,:,285].reshape(14,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f946e7e9-e4f5-45f8-b5e9-179588bb993b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[12.4186,  9.2246,  8.2434,  6.7615,  5.1892,  3.4349,  3.3213,  3.2908,\n",
       "          3.1910,  2.9049]], device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([[285, 281, 282, 287, 284, 283, 289, 293, 785, 292]], device='cuda:0'))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(logits, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f4e5d0-3576-48b7-b86e-691fe4e24111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4425b0-8e2e-48c9-b874-657fd8de9667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
