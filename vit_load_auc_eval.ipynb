{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loaded heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/crc/c/conda/23.5.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTModel, ViTConfig, TrainingArguments, Trainer\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from datasets import load_dataset,load_metric\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: golden retriever\n"
     ]
    }
   ],
   "source": [
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# url = \"http://farm3.staticflickr.com/2066/1798910782_5536af8767_z.jpg\"\n",
    "# url = \"http://farm1.staticflickr.com/184/399924547_98e6cef97a_z.jpg\"\n",
    "url = \"http://farm1.staticflickr.com/128/318959350_1a39aae18c_z.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "pretrained_name = 'google/vit-base-patch16-224'\n",
    "# pretrained_name = 'vit-base-patch16-224-finetuned-imageneteval'\n",
    "# pretrained_name = 'openai/clip-vit-base-patch32'\n",
    "config = ViTConfig.from_pretrained(pretrained_name)\n",
    "processor = ViTImageProcessor.from_pretrained(pretrained_name)\n",
    "# get mean and std to unnormalize the processed images\n",
    "mean, std = processor.image_mean, processor.image_std\n",
    "\n",
    "pred_model = ViTForImageClassification.from_pretrained(pretrained_name)\n",
    "pred_model.to(device)\n",
    "# set to eval mode\n",
    "pred_model.eval()\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "outputs = pred_model(**inputs, output_hidden_states=True)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", pred_model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def load_data(seed=42): \n",
    "    dataset = load_dataset(\"mrm8488/ImageNet1K-val\")\n",
    "    dataset = dataset['train']\n",
    "    splits = dataset.train_test_split(test_size=0.1, seed=seed)\n",
    "    test_ds = splits['test']\n",
    "    splits = splits['train'].train_test_split(test_size=0.1, seed=seed)\n",
    "    train_ds = splits['train']\n",
    "    val_ds = splits['test']\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "_, _, test_ds = load_data()\n",
    "\n",
    "normalize = Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "if \"height\" in processor.size:\n",
    "    size = (processor.size[\"height\"], processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in processor.size:\n",
    "    size = processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = processor.size.get(\"longest_edge\")\n",
    "\n",
    "transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "test_ds.set_transform(preprocess)\n",
    "\n",
    "# batch size is limited to 2, because n_steps could could huge memory consumption\n",
    "batch_size = 1\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:48, 48.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluation import EvalGame\n",
    "import os \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# root_path = 'results/maskgen-vit-1epoch'\n",
    "root_path = 'results/rise-2000-vit'\n",
    "model = lambda x: pred_model(pixel_values=x).logits\n",
    "eval_game = EvalGame(model=lambda x : pred_model(pixel_values=x).logits, output_dim=1000)\n",
    "topk = 10\n",
    "\n",
    "def extract_number(filename):\n",
    "    \"\"\"\n",
    "    Extract the first occurance of countinuous numbers in a file name\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    if match:\n",
    "        return int(match.group(0))\n",
    "    return 0\n",
    "\n",
    "def get_ordered_filename_list(root_path):\n",
    "    file_path_list = []\n",
    "    for filename in os.listdir(root_path):\n",
    "        if filename.endswith('.npy'):\n",
    "            file_path = os.path.join(root_path, filename)\n",
    "            file_path_list.append(file_path)\n",
    "    file_path_list = sorted(file_path_list, key=extract_number)\n",
    "    return file_path_list\n",
    "\n",
    "\n",
    "def load_heatmap(root_path):\n",
    "    file_path_list = get_ordered_filename_list(root_path)\n",
    "    for filename in file_path_list:\n",
    "       yield np.load(filename)\n",
    "\n",
    "\n",
    "ins_score_list = []\n",
    "del_score_list = []\n",
    "\n",
    "heatmap_generator = load_heatmap(root_path)\n",
    "test_dataloader_iterator = iter(test_dataloader)\n",
    "for idx, data in tqdm(enumerate(heatmap_generator)):\n",
    "    heatmap_batch = torch.tensor(data, device=device)\n",
    "\n",
    "    for j in range(100):\n",
    "        with torch.no_grad():\n",
    "            # heatmap = heatmap_batch[j:j+1]\n",
    "            heatmap = torch.randn_like(heatmap_batch[j:j+1])\n",
    "            # Average pooling to convert to 14*14 heatmap\n",
    "            if root_path == 'results/ig-vit':\n",
    "                heatmap = F.avg_pool2d(heatmap, kernel_size=16, stride=16)\n",
    "            input_img = next(test_dataloader_iterator)['pixel_values'].to(device)\n",
    "\n",
    "            pseudo_label = pred_model(input_img).logits.argmax(-1).view(-1)\n",
    "            # ins_score = eval_game.get_insertion_at_topk(input_img, heatmap, topk).detach()\n",
    "            # del_score = eval_game.get_deletion_at_topk(input_img, heatmap, topk).detach()\n",
    "            ins_score = eval_game.get_insertion_score(input_img, heatmap).detach()\n",
    "            del_score = eval_game.get_deletion_score(input_img, heatmap).detach()\n",
    "\n",
    "            # append to list\n",
    "            ins_score_list.append(ins_score)\n",
    "            del_score_list.append(del_score)\n",
    "        if idx * 100 + j >= 100:\n",
    "            print('break')\n",
    "            break\n",
    "    if idx >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(49.5161, device='cuda:0')\n",
      "tensor(51.2999, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ins_score = torch.stack(ins_score_list)\n",
    "del_score = torch.stack(del_score_list)\n",
    "print(ins_score.mean())\n",
    "print(del_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0, 14, 14])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 32 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_overlap_np\n\u001b[1;32m      3\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m----> 5\u001b[0m predicted_class_idx \u001b[38;5;241m=\u001b[39m \u001b[43mpseudo_label\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted class:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mid2label[predicted_class_idx])\n\u001b[1;32m      8\u001b[0m hm \u001b[38;5;241m=\u001b[39m heatmap[idx]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 32 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "from utils.img_utils import plot_overlap_np\n",
    "\n",
    "idx = 32\n",
    "\n",
    "predicted_class_idx = pseudo_label[idx].item()\n",
    "print(\"Predicted class:\", pred_model.config.id2label[predicted_class_idx])\n",
    "\n",
    "hm = heatmap[idx].detach().cpu().numpy()\n",
    "img = input_img[idx].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "plt.title(f\"Method: RISE-1000 Predicted class: {pred_model.config.id2label[predicted_class_idx]}\")\n",
    "plt.annotate(f\"ins_score: {ins_score_list[-1][idx]:.4f}\", (0, 10))\n",
    "plt.annotate(f\"del_score: {del_score_list[-1][idx]:.4f}\", (0, 20))\n",
    "img_int, heatmap_img = plot_overlap_np(img, hm, mean, std)\n",
    "\n",
    "print(\"ins_score:\", ins_score_list[-1][idx])\n",
    "print(\"del_score:\", del_score_list[-1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 14, 14])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
